I"ø/<p>I am often confronted with the problem of trying to <strong>reduce a high dimensional feature set</strong> to a, smaller, more effective one.
Reducing dimension is important for machine learning models as:</p>

<ul>
  <li>the <strong>volume of the ‚Äúsearch space‚Äù grows exponentially</strong>
    <ul>
      <li>at a minimum rate of \(2^{d}\) for binary categorical variables to a much higher exponential for continuous or n-ary categoricals.</li>
    </ul>
  </li>
  <li>the <strong>joint-distribution</strong> of high dimensional empirical spaces tends to be <strong>sparse and ill-defined</strong>
    <ul>
      <li>empirical distributions require data sample size in proportion to the volume of the space (exponential)</li>
      <li>cannot reasonably determine the joint distribution of sparsely supported spaces</li>
    </ul>
  </li>
  <li><strong>combinatorial explosion</strong>
    <ul>
      <li>number of variable combinations can grow to an astronomical # (with n! growth)</li>
      <li>possible sub-space conditional distributions</li>
    </ul>
  </li>
</ul>

<h2 id="objective">Objective</h2>
<p>Assuming we are pursuing a classification problem, the objective of feature selection should be to determine a feature
subspace that <strong>maximizes discrimination between classes</strong> and <strong>minimizes model complexity</strong>.   Alternatively, we can express this
as a minimization of:</p>

\[\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand\given[1][]{\:#1\lvert\:}
\argmin_{ \vec{w} \in \{ set\, of\, all\, \left[ 0,1 \right]^d permutations \} } \sum_{i \in n} loss( f(\vec{w} \cdot \vec{x}_i), y_i) + penalty(\vec{w})\]

<p>where:</p>

<ul>
  <li><strong>loss function</strong> \(loss (f(\vec{x}_i), y_i)\) is the cost of mislabeling where \(f(\vec{x}_i) \ne y_i\)
    <ul>
      <li>there are many different realizations of this function depending on whether is used for regression or classification and particular ML model.  For 
example, for least-squares regression would simply be \(loss(\hat{y}, y) = (\hat{y} - y)^2\)</li>
    </ul>
  </li>
  <li><strong>penalty function</strong> \(penalty (weights)\) is a model complexity penalty
    <ul>
      <li>L2 regularization imposes a \(\lambda \vec{w}^T \vec{w}\) penalty</li>
      <li>L1 regularization imposes a \(\lambda \sum_{i} \vert w_i \vert\), though refactored to avoid the discontinuity of the absolute value function.</li>
    </ul>
  </li>
</ul>

<p>Conceptually we could evaluate all (non-0 magnitude) permutations of the d-dimensional weight vector \(\vec{w}\) where weights are either 0 or 1,
corresponding to enabling or disabling a feature.  Some subset of features, expressed as 0 or 1 weights in \(\vec(w)\) will minimize 
the loss + complexity penalty.</p>

<h2 id="approaches">Approaches</h2>
<p>Here are some approaches:</p>

<ul>
  <li><strong>subspace projections</strong> from dimensionality d down to k, where k &lt; d
    <ul>
      <li>PCA, LDA, ICA, ‚Ä¶</li>
      <li>auto-encoders: model (typically deep-learning) learns to reproduce d-dimensional features from lower dimensional k-dimensional 
synthetic features, in a feedback loop.  The k-dimensional features are the dimensionally reduced output.</li>
    </ul>
  </li>
  <li><strong>ML model based</strong>, using ML model regularization to determine feature relevance
    <ul>
      <li>random forest</li>
      <li>regression / classification with L1 regularization</li>
    </ul>
  </li>
  <li><strong>greedy</strong>
    <ul>
      <li>iterative approaches where subsets of features are selected and evaluated against a particular model</li>
      <li>this is very data and model specific and likely to overfit.</li>
      <li>computationally infeasible to do exhaustively for high dimensional data sets</li>
    </ul>
  </li>
  <li><strong>information geometric</strong> approaches
    <ul>
      <li>evaluate discrimination of prior \(p(feature)\) versus \(p(feature \,\vert\, label)\)</li>
    </ul>
  </li>
</ul>

<p>I have found that the <strong>distributional</strong> and <strong>model based</strong> approaches to be the most effective for my work.  That said,
will start by discussing <strong>subspace projections</strong>.</p>

<h2 id="subspace-projections">Subspace Projections</h2>
<p>Subspace projection finds a transformation on high d-dimensional features, reducing to k-dimensional features + error:</p>

\[\begin{align*}
\vec{f}_k =&amp; T(\vec{f}_d) + \epsilon_k
\end{align*}\]

<p>where \(T()\) is the (lossy) transform from \(\mathbb{R}^d\) to \(\mathbb{R}^k\).</p>

<h3 id="pca">PCA</h3>
<p>PCA is technique that finds orthogonal axes in a data set; the dimensionality of a data set can be reduced by selecting a
subset of these axes, recombining to produce a feature set with reduced dimension.</p>

<p>PCA determines othogonal components in \(\mathbb{R}^d\) using the SVD 
decomposition of the covariance matrix, and determines d eigenvector / eigenvalue pairs, known as ‚Äúprincipal components‚Äù.  Each 
eigenvector defines an orthogonal axis in \(\mathbb{R}^d\) that aligns along maximal variance.  The variance oriented alignment 
is easy to see in the decomposition of a 2-dimensional data set depicted below (source: Wikipedia):</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/440px-GaussianScatterPCA.svg.png" alt="example of PCA (from wikipedia)" /></p>

<p>Dimensional reduction is achieved by selecting the k &lt; d eigenvectors with the highest magnitude (variance).  The original data
can be reconstructed (with some error) from the k eignvectors with a linear combination.</p>

<h4 id="problems-with-pca">Problems with PCA</h4>
<p>However, <strong>PCA is rarely going to be an effective approach</strong> for dimensional reduction in machine learning data sets.  There are two 
fundamental problems with PCA used in this context:</p>

<ol>
  <li>PCA has <strong>no conditioning on outcome</strong>
    <ul>
      <li>should be performed, in some way, on conditonal distribution \(p(x \vert f(x))\)</li>
    </ul>
  </li>
  <li>The variance of features may have <strong>little to do with</strong> \(p(f(x) \vert x)\)
    <ul>
      <li>the transfer function y = f(x) may be non-linear or scaled in such a way that the covariance of x
is not representative.</li>
    </ul>
  </li>
</ol>

<p>Consider the above figure showing the principal component decomposition of a 2 dimensional feature set.  If we use PCA
to reduce from 2 dimensions to 1 dimension, the eigenvector with the largest eigenvalue (the magnitude) would be chosen.  In 
this case the vector aligned at ~30 degrees would be the selected vector.</p>

<p>However, this vector may not be aligned in importance with the underlying function f(x).  In dimensional reduction we
want to select components that align with the largest transfer function (f(x)) sensitivities.  These will correspond to the
partial derivatives of f(x) with respect to each feature (and not necessarily the variance of features).</p>

<p>By way of example, supposing we want to use ML to learn the following function:</p>

\[f(\vec{x}) = 
\begin{bmatrix}
100 &amp; 0 \\
0 &amp; 1
\end{bmatrix}
\begin{bmatrix}
cos (-\pi/6) &amp; -sin (-\pi/6) \\
sin (-\pi/6) &amp; cos (-\pi/6)
\end{bmatrix}
\vec{x}
+ \vec{\epsilon}\]

<p>The above function has 100x the sensitivity to the 2nd PCA component in the features and very little sensitivity to the 1st
PCA component.  In selecting the 1st component (the component with highest variance), PCA has selected a noisy variable 
with little impact on f(x).  Training a model for f(x) on the PCA reduced feature set would produce a very low accuracy,
given that most of the information was in the 2nd component.</p>

<p>While the above example was contrived, in practice I find that PCA often emphasizes the wrong features when performing
dimensional reduction for my data sets.</p>

<h3 id="lda">LDA</h3>
<p>LDA solves some of the problems of PCA, where instead of determining orthogonal components on maximum variance axes,
determines components than maximize linear class-separation.  Sebastian Raschka has written an excellent article on
<a href="https://sebastianraschka.com/Articles/2014_python_lda.html">Linear Discriminant Analysis</a>, contrasting to PCA .  Borrowing a 
diagram from his blog:</p>

<p>
<img src="https://sebastianraschka.com/images/blog/2014/linear-discriminant-analysis/lda_1.png" width="600" height="300" />
(source: Sebastian Raschka's blog)
</p>

<h4 id="deficiencies-of-lda">Deficiencies of LDA</h4>
<p>LDA will often perform better than PCA due to the explicit selection of components that
maximize class separation.  There are, however, some issues that may make it unsuitable for a given data set:</p>

<ul>
  <li><strong>less effective for non-linear relationships</strong>
    <ul>
      <li>if the classes / features are not linearly separable, requiring non-linear separation, this approach will fail</li>
    </ul>
  </li>
  <li>use of <strong>centroids to define barycenter</strong> for each feature distribution
    <ul>
      <li>this is appropriate for relatively symmetric distributions, but would tend to be unrepresentative for many other
distributions present in financial data.</li>
    </ul>
  </li>
</ul>

<h3 id="auto-encoders">Auto-Encoders</h3>
<p>Auto-Encoders present a very interesting approach to dimensional reduction.  The
approach involves creating a <strong>deep neural network</strong> with:</p>

<ul>
  <li><strong>d</strong> inputs (for feature dimension \(\mathbb{R}^d\))</li>
  <li>hidden layers to effect the encoding and decoding</li>
  <li>an internal layer with <strong>k</strong> nodes between the encoder and decoder (to obtain k-dimensional features)</li>
  <li>an output layer with <strong>d</strong> nodes (to reconstitute the d-dimensional input features)</li>
</ul>

<p>The idea is that we want to train the encoder to generate features in \(\mathbb{R}^k\) space, with fidelity to
reproduce the d-dimensional features from the k-dimensional reduced feature set.</p>

<p><img src="/assets/2020-08-13/autoencoder.png" width="350" height="300" /></p>

<p>The network is trained on the same input and outputs, such that the hidden internal k-dimensional layer learns the
optimal representation in \(\mathbb{R}^k\) sufficient to reproduce \(\mathbb{R}^d\) with minimal error.</p>

<p>While deep-learning ANNs allow for non-linearities (unlike PCA), auto-encoders also suffer from inability to classify the relative importance
of features based on feature -&gt; class fidelity.</p>

<h2 id="intermediate-conclusions">Intermediate Conclusions</h2>
<p>Each approach has drawbacks and may or may not be suitable for a given data set:</p>

<ul>
  <li>PCA has significant drawbacks (probably the least effective of the 3):
    <ul>
      <li>negative: cannot select based on discrimination between classes (or outcome)</li>
      <li>negative: linear transformation</li>
    </ul>
  </li>
  <li>LDA improves on PCA:
    <ul>
      <li>positive: determines components that maximize discrimination between classes</li>
      <li>negative: requires linear separability</li>
      <li>negative: use of centroids will fail with non-symmetric distributions</li>
    </ul>
  </li>
  <li>Auto-Encoders:
    <ul>
      <li>positive: can embody non-linearities in the transformation of features</li>
      <li>positive: allow for higher compression of dimension than PCA for certain data sets</li>
      <li>negative: cannot select based on discrimination between classes (or outcome)</li>
    </ul>
  </li>
</ul>

<p>Ideally we want a dimensional reduction technique that:</p>

<ul>
  <li>is aware of the relationship between features and outcome
    <ul>
      <li>choosing features that maximize resolution of p(f(x) = y | x)</li>
    </ul>
  </li>
  <li>can deal with a wide variety of feature distributions
    <ul>
      <li>for example distributions with fat tails, exponential, etc</li>
    </ul>
  </li>
  <li>non-linear</li>
</ul>

<h2 id="next">Next</h2>
<p>In the next post will describe a distribution based approach and compare to a ML model importance based approach.</p>

:ET